<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Taylor Genetic Programming for Symbolic Regression</title>
    <link href="/TaylorGP/"/>
    <url>/TaylorGP/</url>
    
    <content type="html"><![CDATA[<p>Baihe He, Qiang Lu, Qingyun Yang, Jake Luo, and Zhiguang Wang.</p><p>Received:  24 March 2022 / Accepted: 14 April 2022.</p><h1 id="TaylorGP"><a href="#TaylorGP" class="headerlink" title="TaylorGP"></a>TaylorGP</h1><p>The paper  <a href="https://github.com/KGAE-CUP/TaylorGP/blob/main/img/Taylor_Symbolic_Regression_GECCO2022.pdf">Taylor Genetic Programming for Symbolic Regression</a>  has been accepted by <a href="https://gecco-2022.sigevo.org/HomePage">GECCO-2022</a> . You could also see our <a href="https://github.com/KGAE-CUP/TaylorGP/blob/main/img/Appendix_Taylor_Symbolic_Regression_GECCO2022.pdf">appendix</a> for more details.</p><h2 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1.Abstract"></a>1.Abstract</h2><p>Genetic programming (GP) is a commonly used approach to solve symbolic regression (SR) problems. Compared with the machine learning or deep learning methods that depend on the pre-defined model and the training dataset for solving SR problems, GP is more focused on finding the solution in a search space. Although GP has good performance on large-scale benchmarks, it randomly transforms individuals to search results without taking advantage of the characteristics of the dataset.So, the search process of GP is usually slow, and the final results could be unstable. To guide GP by these characteristics, we propose a new method for SR, called Taylor genetic programming (TaylorGP). TaylorGP leverages a Taylor polynomial to approximate the symbolic equation that fits the dataset. It also utilizes the Taylor polynomial to extract the features of the symbolic equation: low order polynomial discrimination, variable separability, boundary, monotonic, and parity. GP is enhanced by these Taylor polynomial techniques. Experiments are conducted on three kinds of benchmarks: classical SR, machine learning, and physics. The experimental results show that TaylorGP not only has higher accuracy than the nine baseline methods, but also is faster in finding stable results.</p><h2 id="2-Code"><a href="#2-Code" class="headerlink" title="2. Code"></a>2. Code</h2><p>You could get the code from our <a href="https://github.com/KGAE-CUP/TaylorGP">github</a>.</p><h3 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h3><p>Make sure you have installed the following python version and pacakges before start running our code:</p><ul><li>python3.6~3.8</li><li>scikit-learn </li><li>numpy </li><li>sympy </li><li>pandas </li><li>time </li><li>copy </li><li>itertools </li><li>timeout_decorator </li><li>scipy </li><li>joblib </li><li>numbers </li><li>itertools </li><li>abc </li><li>warnings </li><li>math</li></ul><p>Our experiments were running in Ubuntu 18.04 with Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz. </p><h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3><p>We provide an example to test whether the module required by Taylor GP is successfully installed: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python TaylorGP.py<br></code></pre></td></tr></table></figure><p>In addition, you can run the specified dataset through the following method: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python TaylorGP.py --fileName=<span class="hljs-string">&quot;Feynman/F24.tsv&quot;</span><br></code></pre></td></tr></table></figure><h2 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3. Experiments"></a>3. Experiments</h2><h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>We evaluate the performance of TaylorGP on three kinds of benchmarks: classical Symbolic Regression Benchmarks (<strong>SRB</strong>), Penn<br>Machine Learning Benchmarks (<strong>PMLB</strong>), and Feynman Symbolic Regression Benchmarks (<strong>FSRB</strong>) .(You could get them from directories GECCO, PMLB and Feynman respectively).The distribution of the total 81 benchmark sizes by samples and features is shown in the following. </p><img src="/img/img_TaylorGP/datasets_size.png" width="50%"><p>The details of these benchmarks are listed in the <a href="https://github.com/KGAE-CUP/TaylorGP/blob/main/img/Appendix_Taylor_Symbolic_Regression_GECCO2022.pdf">appendix</a>.</p><h3 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h3><p>We compare TaylorGP with two kinds of baseline algorithms \footnote{The nine baseline algorithms are implemented in <a href="https://github.com/cavalab/srbench">SRBench</a> : four symbolic regression methods and five machine learning methods. The symbolic regression methods include <a href="https://github.com/trevorstephens/gplearn"><strong>GPlearn</strong></a>, FFX , geometric semantic genetic programming (<strong>GSGP</strong>) and bayesian symbolic regression (<strong>BSR</strong>). The machine learning methods include linear regression (<strong>LR</strong>), kernel ridge regression (<strong>KR</strong>), random forest regression (<strong>RF</strong>), support vector machines (<strong>SVM</strong>), and <strong>XGBoost</strong> . </p><p>As shown in the figure below , we illustrate the normalized <strong>R^2 scores</strong> of the ten algorithms running 30 times on all benchmarks. Since the normalized R^2 closer to 1 indicates better results, overall TaylorGP can find more accurate results than other algorithms.</p><img src="/img/img_TaylorGP/contact.jpg" width="50%"><p>Normalized R^2 comparisons of the ten SR methods on <strong>classical Symbolic Regression Benchmarks</strong></p><img src="/img/img_TaylorGP/GECCO.jpg" width="50%"><p>Normalized R^2 comparisons of the ten SR methods on <strong>Feynman Symbolic Regression Benchmarks</strong></p><img src="/img/img_TaylorGP/AIFeynman.jpg" width="50%"><p>Normalized R^2 comparisons of the ten SR methods on <strong>Penn Machine Learning Benchmarks</strong></p><img src="/img/img_TaylorGP/ML_father.jpg" width="50%">]]></content>
    
    
    <categories>
      
      <category>Evolutionary computation and Symbolic regression</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Symbolic regression</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Exploring Hidden Semantics in Neural Networks with Symbolic Regression</title>
    <link href="/SRNet/"/>
    <url>/SRNet/</url>
    
    <content type="html"><![CDATA[<p>Yuanzhen Luo, Qiang Lu (<a href="mailto:&#x6c;&#x75;&#113;&#x69;&#x61;&#110;&#x67;&#x40;&#x63;&#x75;&#112;&#x2e;&#101;&#x64;&#x75;&#46;&#x63;&#110;">&#x6c;&#x75;&#113;&#x69;&#x61;&#110;&#x67;&#x40;&#x63;&#x75;&#112;&#x2e;&#101;&#x64;&#x75;&#46;&#x63;&#110;</a>), Xilei Hu, Jake Luo and Zhiguang Wang.</p><p>Received:  24 March 2022 / Accepted: 14 April 2022 GECCO</p><h1 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h1><ul><li>Code: <a href="https://github.com/KGAE-CUP/SRNet-GECCO">https://github.com/KGAE-CUP/SRNet-GECCO</a></li><li>Paper: <a href="https://github.com/KGAE-CUP/SRNet-GECCO/blob/master/Exploring_Hidden_Semantics_in_Neural_Networks_with_Symbolic_Regression_GECCO_22.pdf">Exploring Hidden Semantics in Neural Networks with Symbolic Regression</a></li><li>Appendix: <a href="https://github.com/KGAE-CUP/SRNet-GECCO/blob/master/appendix.pdf">Supplementary Material</a></li></ul><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Many recent studies focus on developing mechanisms to explain the black-box behaviors of neural networks (NNs). However, little work has been done to extract the potential hidden semantics (mathematical representation) of a neural network. A succinct and explicit mathematical representation of a NN model could improve the understanding and interpretation of its behaviors. To address this need, we propose a novel symbolic regression method for neural works (called SRNet) to discover the mathematical expressions of a NN. SRNet creates a Cartesian genetic programming (NNCGP) to represent the hidden semantics of a single layer in a NN. It then leverages a multi-chromosome NNCGP to represent hidden semantics of all layers of the NN. The method uses a (1+$\lambda$) evolutionary strategy (called MNNCGP-ES) to extract the final mathematical expressions of all layers in the NN. Experiments on 12 symbolic regression benchmarks and 5 classification benchmarks show that SRNet not only can reveal the complex relationships between each layer of a NN but also can extract the mathematical representation of the whole NN. Compared with LIME and MAPLE, SRNet has higher interpolation accuracy and trends to approximate the real model on the practical dataset</p><h1 id="SRNet-Model"><a href="#SRNet-Model" class="headerlink" title="SRNet Model"></a>SRNet Model</h1><p>We present hidden semantics h of each layer in a NN as: </p><img src="/img/SRNet/f.png"><p>where f is represented by a CGP model, and w and b are a weight vector, as shown in figure:</p><p><img src="/img/SRNet/CGPSRNet.png" alt="SRNet Model"> </p><p>For more details about our model and methods, please see our <a href="https://github.com/KGAE-CUP/SRNet-GECCO/blob/master/Exploring_Hidden_Semantics_in_Neural_Networks_with_Symbolic_Regression_GECCO_22.pdf">paper</a>.</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>We test our model at 12 regression tasks (SR Benchmarks and physical laws) and 6 classification tasks (PMLB), as shown in figure:</p><p><img src="/img/SRNet/dataset_plot.jpg" alt="Dataset"></p><p>All the experiments code at <a href="https://github.com/KGAE-CUP/SRNet-GECCO">here</a>. Here we show our experimental figures in our paper:</p><h2 id="Convergence"><a href="#Convergence" class="headerlink" title="Convergence"></a>Convergence</h2><p>The convergence curves for all dataset:</p><p><img src="/img/SRNet/reg_trend.png" alt="Regression convergence curve of fitness"></p><p><img src="/img/SRNet/clas_trend.png" alt="Classification convergence curve of fitness"></p><h2 id="Semantics-Mathematical-Expressions"><a href="#Semantics-Mathematical-Expressions" class="headerlink" title="Semantics (Mathematical Expressions)"></a>Semantics (Mathematical Expressions)</h2><p><img src="/img/SRNet/hidden_semantics.png" alt="Hidden Semantics"></p><p>Combining these expressions, we can obtain the overall expressions for all NNs:</p><p><img src="/img/SRNet/semantics.png" alt="Semantics"></p><h2 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h2><p>We compare SRNet to LIME and MAPLE on both regression and classification tasks.</p><h3 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h3><p><img src="/img/SRNet/reg_comparison.png" alt="Regression Comparison"></p><h3 id="Classifiation"><a href="#Classifiation" class="headerlink" title="Classifiation"></a>Classifiation</h3><p>Decision boundary:</p><p><img src="/img/SRNet/local_db.png" alt="DB of SRNet vs. LIME vs. MAPLE"></p><p>Accuracy:</p><p><img src="/img/SRNet/accs.png" alt="Acc of SRNet vs. LIME vs. MAPLE"></p><h1 id="Cite"><a href="#Cite" class="headerlink" title="Cite"></a>Cite</h1><ul><li><p>Cite this page:</p><p>  <a href="https://kgae-cup.github.io/SRNet">https://kgae-cup.github.io/SRNet</a></p></li><li><p>Cite this paper:</p>  <figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">@inproceedings&#123;Yuanzhen2022s,<br>    title=&#123;Exploring Hidden Semantics in Neural Networks with Symbolic Regression&#125;,<br>    author=&#123;Yuanzhen, Luo <span class="hljs-keyword">and </span>Qiang, Lu <span class="hljs-keyword">and </span>Xilei, Hu <span class="hljs-keyword">and </span><span class="hljs-keyword">Jake, </span>Luo <span class="hljs-keyword">and </span>Zhiguang, Wang&#125;,<br>    <span class="hljs-keyword">booktitle=&#123;Proceedings </span>of the Genetic <span class="hljs-keyword">and </span>Evolutionary Computation Conference&#125;,<br>    pages=&#123;xxx--xxx&#125;,<br>    year=&#123;<span class="hljs-number">2022</span>&#125;<br>&#125;<br></code></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>Evolutionary computation and Symbolic regression</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Symbolic regression, neural networks</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Trajectory Splicing</title>
    <link href="/Trajectory-Splicing/"/>
    <url>/Trajectory-Splicing/</url>
    
    <content type="html"><![CDATA[<p>Qiang Lu · Rencai Wang · Bin Yang · Zhiguang Wang.</p><p>Received:  23 September 2018 / 30 June 2019</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>With continued development of location-based systems, large amounts of trajectories become<br>available which record moving objects’ locations across time. If the trajectories collected by<br>different location-based systems come from the same moving object, they are spliceable<br>trajectories, which contribute to representing holistic behaviors of the moving object. In this<br>paper, we consider how to efficiently identify spliceable trajectories. More specifically, we<br>first formalize a spliced model to capture spliceable trajectories where their times are disjoint,<br>and the distances between them are close. Next, to efficiently implement the model, we design<br>three components: a disjoint time index, a directed acyclic graph of sub-trajectory location<br>connections, and two splice algorithms. The disjoint time index saves a disjoint time set of<br>each trajectory for querying disjoint time trajectories efficiently. The directed acyclic graph<br>contributes to discovering groups of spliceable trajectories. Based on the identified groups,<br>the splice algorithm findmaxCTR finds maximal groups containing all spliceable trajectories.<br>Although the splice algorithm is efficient in some practical applications, its running time is<br>exponential. Therefore, an approximate algorithm findApproxMaxCTR is proposed to find<br>trajectories which can be spliced with each other with a certain probability within polynomial<br>run time. Finally, experiments on two datasets demonstrate that the model and its components<br>are effective and efficient.</p><h2 id="More-info-about-thie-paper"><a href="#More-info-about-thie-paper" class="headerlink" title="More info about thie paper"></a>More info about thie paper</h2><p>Please see <a href="https://www.cup.edu.cn/cise/docs/2021-04/99681f7bd5cc4f168be06704098772e4.pdf">KAIS-Link</a></p><h2 id="Code-Link"><a href="#Code-Link" class="headerlink" title="Code Link"></a>Code Link</h2><p>Please see <a href="https://github.com/KGAE-CUP">Code</a> on our GitHub.</p>]]></content>
    
    
    <categories>
      
      <category>Trajectory Analysis and Mining</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spatiotemporal Trajectory</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A semantic annotation based software knowledge sharing space</title>
    <link href="/A-semantic-annotation-based-software-knowledge-sharing-space/"/>
    <url>/A-semantic-annotation-based-software-knowledge-sharing-space/</url>
    
    <content type="html"><![CDATA[<p>Qiang Lu; Ming Chen; Zhiguang Wang.</p><p>Received: 2008 </p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Over the past several years, the software engineering needs large number of developers as the scale of software increasing. Therefore, the collaborative work and knowledge communication become difficult between team members. In order to improve knowledge sharing in team members, a semantic annotation based software knowledge sharing space (SKSS) is proposed. For implementing SKSS, at first, the knowledge in the process of software development is analyzed. Then, the SKSS is formalized based on the above analysis, and SKSS ontology is created. At last, the framework of semantic annotation is imported to implement SKSS, and all knowledge in software team will be connected into a completed knowledge net based on the framework.</p><h2 id="More-info-about-thie-paper"><a href="#More-info-about-thie-paper" class="headerlink" title="More info about thie paper"></a>More info about thie paper</h2><p>Please see <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4663375">IEEE-Link</a></p><h2 id="Code-Link"><a href="#Code-Link" class="headerlink" title="Code Link"></a>Code Link</h2><p>Please see <a href="https://github.com/KGAE-CUP">Code</a> on our GitHub.</p>]]></content>
    
    
    <categories>
      
      <category>knowledge Graphs and Intelligent Question Answering</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Natural Language Processing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An ant colony optimization algorithm for the one-dimensional cutting stock problem with multiple stock lengths</title>
    <link href="/An-ant-colony-optimization-algorithm-for-the-one-dimensional-cutting-stock-problem-with-multiple-stock-lengths/"/>
    <url>/An-ant-colony-optimization-algorithm-for-the-one-dimensional-cutting-stock-problem-with-multiple-stock-lengths/</url>
    
    <content type="html"><![CDATA[<p>Qiang Lu, Zhiguang Wang, Ming Chen.</p><p>Received: 2008 </p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>The Cutting Stock Problem (CSP) with multiple stock lengths is the NP-hard combinatorial optimization problem. In recent years, the CSP is researched by applying evolutionary approaches which includes Genetic Algorithm, Evolutionary Programming, et al. In the paper, an ant colony optimiation (ACO) algorithm for one-dimensional cutting stock problems with muliple stock lengths(MCSP) is presented, and mutation operatation is imported into the ACO in order to avoid the phenomenon of precocity and stagnation emerging. Based on the analysis of the algorithm, the ACO for MCSP has the same time complexity as CSP. Throught exmperiments study, the outcome shows that, compared with other algorithm, the algorithm takes a great improvement in the convergent speed and result optimization.</p><h2 id="More-info-about-thie-paper"><a href="#More-info-about-thie-paper" class="headerlink" title="More info about thie paper"></a>More info about thie paper</h2><p>Please see <a href="https://dl.acm.org/doi/10.1109/ICNC.2008.208">ACM-Link</a></p><h2 id="Code-Link"><a href="#Code-Link" class="headerlink" title="Code Link"></a>Code Link</h2><p>Please see <a href="https://github.com/KGAE-CUP">Code</a> on our GitHub.</p>]]></content>
    
    
    <categories>
      
      <category>Evolutionary computation and Symbolic regression</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Evolutionary Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Using genetic programming with prior formula knowledge to solve symbolic regression problem. Computational intelligence and neuroscience</title>
    <link href="/Using%20genetic%20programming%20with%20prior%20formula%20knowledge%20to%20solve%20symbolic%20regression%20problem.%20Computational%20intelligence%20and%20neuroscience%20-%20%E5%89%AF%E6%9C%AC%20-%20%E5%89%AF%E6%9C%AC/"/>
    <url>/Using%20genetic%20programming%20with%20prior%20formula%20knowledge%20to%20solve%20symbolic%20regression%20problem.%20Computational%20intelligence%20and%20neuroscience%20-%20%E5%89%AF%E6%9C%AC%20-%20%E5%89%AF%E6%9C%AC/</url>
    
    <content type="html"><![CDATA[<p>Qiang Lu, Jun Ren, and Zhiguang Wang.</p><p>Received:  28 May 2015 / Accepted: 5 October 2015.</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>A researcher can infer mathematical expressions of functions quickly by using his professional knowledge (called Prior Knowledge). But the results he finds may be biased and restricted to his research field due to limitation of his knowledge. In contrast, Genetic Programming method can discover fitted mathematical expressions from the huge search space through running evolutionary algorithms. And its results can be generalized to accommodate different fields of knowledge. However, since GP has to search a huge space, its speed of finding the results is rather slow. Therefore, in this paper, a framework of connection between Prior Formula Knowledge and GP (PFK-GP) is proposed to reduce the space of GP searching. The PFK is built based on the Deep Belief Network (DBN) which can identify candidate formulas that are consistent with the features of experimental data. By using these candidate formulas as the seed of a randomly generated population, PFK-GP finds the right formulas quickly by exploring the search space of data features. We have compared PFK-GP with Pareto GP on regression of eight benchmark problems. The experimental results confirm that the PFK-GP can reduce the search space and obtain the significant improvement in the quality of SR.</p><h2 id="More-info-about-thie-paper"><a href="#More-info-about-thie-paper" class="headerlink" title="More info about thie paper"></a>More info about thie paper</h2><p>Please see <a href="https://downloads.hindawi.com/journals/cin/2016/1021378.pdf">Link</a></p><h2 id="Code-Link"><a href="#Code-Link" class="headerlink" title="Code Link"></a>Code Link</h2><p>Please see <a href="https://github.com/KGAE-CUP">Code</a> on our GitHub.</p>]]></content>
    
    
    <categories>
      
      <category>Evolutionary computation and Symbolic regression</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Symbolic regression</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Enhancing Gene Expression Programming based on Space Partition and Jump for Symbolic Regression, Information Sciences</title>
    <link href="/Enhancing-Gene-Expression-Programming-based-on-Space-Partition-and-Jump-for-Symbolic-Regression-Information-Sciences/"/>
    <url>/Enhancing-Gene-Expression-Programming-based-on-Space-Partition-and-Jump-for-Symbolic-Regression-Information-Sciences/</url>
    
    <content type="html"><![CDATA[<p>Lu Q, Zhou S, Tao F, Luo J, Wang ZG.</p><p>Received: 23 April 2019 / Accepted: 18  August 2020.</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>When solving a symbolic regression problem, the gene expression programming (GEP)algorithm could fall into a premature convergence which terminates the optimization pro-cess too early, and may only reach a poor local optimum. To address the premature conver-gence problem of GEP, we propose a novel algorithm named SPJ-GEP, which can maintainthe GEP population diversity and improve the accuracy of the GEP search by allowing thepopulation to jump efficiently between segmented subspaces. SPJ-GEP first divides thespace of mathematical expressions intoksubspaces that are mutually exclusive. It thencreates a subspace selection method that combines the multi-armed bandit and the-greedy strategy to choose a jump subspace. In this way, the analysis is made on the pop-ulation diversity and the range of the number of subspaces. The analysis results show thatSPJ-GEP does not significantly increase the computational complexity of time and spacethan classical GEP methods. Besides, an evaluation is conducted on a set of standard SRbenchmarks. The evaluation results show that the proposed SPJ-GEP keeps a higher popu-lation diversity and has an enhanced accuracy compared with three baseline GEP methods.</p><h2 id="More-info-about-thie-paper"><a href="#More-info-about-thie-paper" class="headerlink" title="More info about thie paper"></a>More info about thie paper</h2><p>Please see <a href="https://www.sciencedirect.com/science/article/pii/S0020025520308276">Sciencedirect-Link</a></p><h2 id="Code-Link"><a href="#Code-Link" class="headerlink" title="Code Link"></a>Code Link</h2><p>Please see <a href="https://github.com/KGAE-CUP">Code</a> on our GitHub.</p>]]></content>
    
    
    <categories>
      
      <category>Evolutionary computation and Symbolic regression</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Symbolic regression</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>About KGAE Lab</title>
    <link href="/About-KGAE-Lab/"/>
    <url>/About-KGAE-Lab/</url>
    
    <content type="html"><![CDATA[<h1 id="Welcome-to-KGAE-Lab"><a href="#Welcome-to-KGAE-Lab" class="headerlink" title="Welcome to KGAE Lab !"></a>Welcome to <a href="https://mp.weixin.qq.com/s/urEqo_WQ56_W7Iitugt8ag">KGAE</a> Lab !</h1><h1 id="Laboratory-introduction"><a href="#Laboratory-introduction" class="headerlink" title="Laboratory introduction"></a>Laboratory introduction</h1><p>  KGAE laboratory currently belongs to the College of information science and engineering, China University of Petroleum (Beijing). The knowledge Graph intelligent performance laboratory is mainly engaged in the research of evolutionary intelligence and evolutionary learning, knowledge Graph and Data Mining. With the goal of “evolving to create intelligence”, our laboratory explores the origin of intelligence, and pays special attention to the combination of evolutionary algorithms and other related intelligent disciplines to deal with coincidence Symbolic regression problems and evolutionary robot problems. In addition, we also studies the relationship between “knowledge and intelligence”, especially the work on knowledge discovery and intelligent question answering on the knowledge Graph.</p><p> Current research direction of the laboratory:<br><strong>1) Symbolic regression</strong><br><strong>2) Evolutionary robot</strong><br><strong>3) Knowledge Graph and intelligent Q &amp; A</strong><br><strong>4) Natural language processing – Text Analysis</strong><br><strong>5) Spatiotemporal data analysis and mining</strong></p><p>Email for specific matters: <a href="mailto:&#108;&#x75;&#x71;&#105;&#97;&#110;&#x67;&#x40;&#99;&#x75;&#x70;&#x2e;&#101;&#100;&#117;&#46;&#x63;&#x6e;">&#108;&#x75;&#x71;&#105;&#97;&#110;&#x67;&#x40;&#99;&#x75;&#x70;&#x2e;&#101;&#100;&#117;&#46;&#x63;&#x6e;</a></p><h1 id="Related-links"><a href="#Related-links" class="headerlink" title="Related links"></a>Related links</h1><p>Google Scholar :  <a href="https://scholar.google.com/citations?user=m61aeIAAAAAJ&amp;hl=zh-CN&amp;oi=sra">https://scholar.google.com/citations?user=m61aeIAAAAAJ&amp;hl=zh-CN&amp;oi=sra</a></p><p>E-mail : <a href="mailto:&#x6c;&#117;&#113;&#105;&#x61;&#x6e;&#103;&#x40;&#99;&#117;&#x70;&#x2e;&#x65;&#100;&#x75;&#46;&#x63;&#x6e;">&#x6c;&#117;&#113;&#105;&#x61;&#x6e;&#103;&#x40;&#99;&#117;&#x70;&#x2e;&#x65;&#100;&#x75;&#46;&#x63;&#x6e;</a>; <a href="mailto:&#x6c;&#x75;&#113;&#x69;&#x61;&#110;&#x67;&#116;&#x6f;&#110;&#121;&#64;&#103;&#109;&#97;&#105;&#x6c;&#x2e;&#99;&#x6f;&#109;">&#x6c;&#x75;&#113;&#x69;&#x61;&#110;&#x67;&#116;&#x6f;&#110;&#121;&#64;&#103;&#109;&#97;&#105;&#x6c;&#x2e;&#99;&#x6f;&#109;</a></p>]]></content>
    
    
    <categories>
      
      <category>Laboratory profile</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Instructions</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Incorporating Actor-Critic in Monte Carlo tree search for symbolic regression</title>
    <link href="/Incorporating%20Actor-Critic%20in%20Monte%20Carlo%20tree%20search%20for%20symbolic/"/>
    <url>/Incorporating%20Actor-Critic%20in%20Monte%20Carlo%20tree%20search%20for%20symbolic/</url>
    
    <content type="html"><![CDATA[<p>Qiang Lu, Fan Tao, Shuo Zhou &amp; Zhiguang Wang.</p><p>Received: 29 May 2020 / Accepted: 11 December 2020.</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Most traditional genetic programming methods that handle symbolic regression are random algorithms without memory and direction. They repeatedly search for the same or similar positions in the symbolic space, and easily fall into premature convergence. To overcome these shortcomings, we propose a new symbolic expression search algorithm based on the Monte Carlo tree search named SE-MCTS. It creates a tree to represent the symbolic space and remembers its visiting positions. Moreover, it incorporates two neural networks—Actor and Critic into the upper confidence bound to direct its search based on the given dataset features and decide when to use particle swarm optimization to find fitted constants in a mathematical expression. The experiment results show that SE-MCTS can discover more proper mathematical expressions than canonical genetic programming methods.</p><h2 id="More-info-about-thie-paper"><a href="#More-info-about-thie-paper" class="headerlink" title="More info about thie paper"></a>More info about thie paper</h2><p>Please see <a href="https://link.springer.com/article/10.1007/s00521-020-05602-2">Springer-Link</a></p><h2 id="Code-Link"><a href="#Code-Link" class="headerlink" title="Code Link"></a>Code Link</h2><p>Please see <a href="https://github.com/KGAE-CUP/SE-MCTS">SE-MCTS</a> on our GitHub.</p>]]></content>
    
    
    <categories>
      
      <category>Evolutionary computation and Symbolic regression</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Symbolic regression</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
